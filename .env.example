# ============================================================================
# Environment Configuration Example
# ============================================================================
#
# Usage:
#   Development:  cp .env.example .env
#   Production:   Update values appropriately for your deployment
#
# Security Note:
#   - Never commit .env with real credentials to version control
#   - Use secrets management in production (Kubernetes Secrets, AWS Secrets Manager)
#
# ============================================================================

# ============================================================================
# Application Settings
# ============================================================================

# Environment: dev, staging, prod
APP_ENV=dev
APP_HOST=0.0.0.0
APP_PORT=8000
APP_DEBUG=false

# ============================================================================
# Model Configuration
# ============================================================================

# Path to trained model directory
# Development: ./models/router
# Production:  /app/models/router (or cloud storage path)
MODEL_PATH=./models/router

# Device for inference: cpu, cuda, mps
# Use 'cuda' if GPU is available for better throughput
MODEL_DEVICE=cpu

# Maximum token length for input sequences
MODEL_MAX_LENGTH=512

# ============================================================================
# Batching Configuration (Phase 2)
# ============================================================================

# Maximum batch size before forced processing (1-256)
# Higher values = better throughput, higher latency
BATCH_MAX_SIZE=32

# Maximum wait time in milliseconds before processing batch (1-1000)
# Lower values = lower latency, smaller batches
BATCH_MAX_WAIT_MS=10

# ============================================================================
# Cache Configuration
# ============================================================================

# L1 Cache (Local LRU) - In-memory cache
# Size: Maximum number of entries (0 to disable)
# Memory usage: ~100 bytes per entry (10,000 entries â‰ˆ 1MB)
CACHE_L1_SIZE=10000

# L1 Cache TTL in seconds
CACHE_L1_TTL_SEC=300

# L2 Cache (Redis) - Optional distributed cache
# Uncomment to enable Redis caching across multiple instances
# REDIS_URL=redis://localhost:6379/0
# CACHE_L2_TTL_SEC=3600

# ============================================================================
# Logging
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Log format: json (production), console (development)
LOG_FORMAT=json

# ============================================================================
# Routing Configuration
# ============================================================================

# Confidence threshold for routing decisions (0.0-1.0)
CONFIDENCE_THRESHOLD=0.7

# ============================================================================
# Rate Limiting (Phase 5 - Optional)
# ============================================================================

# Enable rate limiting to prevent abuse
RATE_LIMIT_ENABLED=false

# Maximum requests per second per client
RATE_LIMIT_REQUESTS_PER_SECOND=100

# ============================================================================
# Production-Only Settings (Uncomment when deploying)
# ============================================================================

# Redis for distributed caching (production)
# REDIS_URL=redis://redis-cluster.production.svc:6379/0

# Cloud Storage for Models (AWS)
# AWS_REGION=us-west-2
# S3_MODEL_BUCKET=my-models-bucket
# S3_MODEL_KEY=semantic-router/v1/router

# Cloud Storage for Models (GCP)
# GCP_PROJECT=my-project
# GCS_MODEL_BUCKET=my-models-bucket
# GCS_MODEL_PATH=semantic-router/v1/router

# Kubernetes Metadata
# POD_NAME=${HOSTNAME}
# POD_NAMESPACE=default

# Observability (OpenTelemetry)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317

# API Authentication (if implemented)
# API_KEY_HEADER=X-API-Key
# API_KEYS=key1,key2,key3  # Use secrets manager in production

# CORS Configuration (if needed)
# CORS_ORIGINS=https://app.example.com,https://dashboard.example.com
# CORS_ALLOW_CREDENTIALS=true

# Performance Tuning
# TORCH_NUM_THREADS=4
# OMP_NUM_THREADS=4

# ============================================================================
# Notes
# ============================================================================
#
# Resource Requirements (per instance):
#   - CPU: 1-2 cores
#   - Memory: 2-4 GB (depends on model size and cache)
#   - Disk: 1-2 GB (for model files)
#
# Scaling Guidelines:
#   - Horizontal: Add instances behind load balancer
#   - Vertical: Increase CPU/memory for larger batches
#   - GPU: Use MODEL_DEVICE=cuda for higher throughput
#
# Monitoring Endpoints:
#   - Health: /health/live, /health/ready
#   - Metrics: /metrics (Prometheus format)
#
# ============================================================================
